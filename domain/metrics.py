import numpy as np
import torch
import sklearn.metrics
from gpytorch.metrics import mean_standardized_log_loss, negative_log_predictive_density, quantile_coverage_error

def gaussian_log_likelihood(model, X, y):
    # generated by GPT4
    model.eval()
    with torch.no_grad():
        output = model(X)
        predictive_mean = output.mean
        predictive_var = output.variance
        error = y - predictive_mean
        log_likelihood = -0.5 * torch.log(2 * np.pi * predictive_var) - 0.5 * (error**2) / predictive_var
    return log_likelihood

def score_ll(log_likelihood, x):
    # generated by GPT4
    #return np.sum(np.log(np.mean(np.exp(log_likelihood), axis=0)))
    # compute the partial derivative of the log likelihood with respect to the hyperparameters
    return np.sum(np.log(np.mean(np.exp(log_likelihood), axis=0))) - np.sum(np.var(log_likelihood, axis=0))

def BIC(sigma_squared, n, p):
    """
    Bayesian Information Criterion
    sigma_squared: mean squared error or variance
    n: number of observations
    p: number of parameters
    """
    return np.log(sigma_squared) + np.log(n) * p

def AIC(sigma_squared, n, p):
    return np.log(sigma_squared) + 2 * p

def VAIC(sigma_squared, n, p):
    return 1 - (np.log(sigma_squared) + 2 * p) / (np.log(sigma_squared) + np.log(n) * p)

def r2_adj(r2, n, k):
    return 1 - (1 - r2) * (n - 1) / (n - k - 1)

def get_metrics(pred_dist, y_true, y_pred, type="regression"):
    if type == "regression":
        return {
            "mse": sklearn.metrics.mean_squared_error(y_true, y_pred),
            "root_mse": sklearn.metrics.mean_squared_error(y_true, y_pred, squared=False),
            "mae": sklearn.metrics.mean_absolute_error(y_true, y_pred),
            "mape": sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred),
            "r2": sklearn.metrics.r2_score(y_true, y_pred),
            "r2_adj": r2_adj(sklearn.metrics.r2_score(y_true, y_pred), len(y_true), len(y_pred)),
            "BIC": BIC(sklearn.metrics.mean_squared_error(y_true, y_pred), len(y_true), len(y_pred)),
            "AIC": AIC(sklearn.metrics.mean_squared_error(y_true, y_pred), len(y_true), len(y_pred)),
            "explained_variance (ESS)": sklearn.metrics.explained_variance_score(y_true, y_pred),
        }
    elif type == "GP":
        return {
            "MSE": sklearn.metrics.mean_squared_error(np.array(y_true), np.array(y_pred)),
            "mean_squared_log_error": sklearn.metrics.mean_squared_log_error(y_true, y_pred),
            "MAPE": sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred),
            "r2": sklearn.metrics.r2_score(y_true, y_pred),
            "explained_variance": sklearn.metrics.explained_variance_score(y_true, y_pred),
            "mean_standardized_log_loss": mean_standardized_log_loss(pred_dist, y_true, y_pred),
            "negative_log_predictive_density": negative_log_predictive_density(pred_dist, y_true),
            #"quantile_coverage_error": quantile_coverage_error(pred_dist, y_true, quantile=0.95),
            "kernel lengthscales": pred_dist.mean_module.base_kernel.lengthscale.item(),
        }
    elif type == "classification":
        return {
            "accuracy": sklearn.metrics.accuracy_score(y_true, y_pred),
            "precision": sklearn.metrics.precision_score(y_true, y_pred),
            "recall": sklearn.metrics.recall_score(y_true, y_pred),
            "f1": sklearn.metrics.f1_score(y_true, y_pred),
            "auc": sklearn.metrics.roc_auc_score(y_true, y_pred),
            "mcc": sklearn.metrics.matthews_corrcoef(y_true, y_pred)
        }

