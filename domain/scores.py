import numpy as np
import torch
import sklearn.metrics
from gpytorch.metrics import mean_standardized_log_loss, negative_log_predictive_density, quantile_coverage_error
from gpytorch.distributions import MultivariateNormal

def gaussian_log_likelihood(model, X, y):
    # generated by GPT4
    model.eval()
    with torch.no_grad():
        output = model(X)
        predictive_mean = output.mean
        predictive_var = output.variance
        error = y - predictive_mean
        log_likelihood = -0.5 * torch.log(2 * np.pi * predictive_var) - 0.5 * (error**2) / predictive_var
    return log_likelihood

def score_ll(log_likelihood, x):
    # generated by GPT4
    #return np.sum(np.log(np.mean(np.exp(log_likelihood), axis=0)))
    # compute the partial derivative of the log likelihood with respect to the hyperparameters
    return np.sum(np.log(np.mean(np.exp(log_likelihood), axis=0))) - np.sum(np.var(log_likelihood, axis=0))

def get_BIC(log_lik, n, p):
    """
    Bayesian Information Criterion
    sigma_squared: mean squared error or variance
    n: number of observations
    p: number of parameters
    """
    if log_lik.shape == (p, n):
        log_lik = np.sum(log_lik)
    return np.log(n) * p - 2 * log_lik

def AIC(sigma_squared, n, p):
    return np.log(sigma_squared) + 2 * p

def VAIC(sigma_squared, n, p):
    return 1 - (np.log(sigma_squared) + 2 * p) / (np.log(sigma_squared) + np.log(n) * p)

def waic(model, likelihood, X, Y):
    "wideley applicable information criterion"
    model.eval()
    with torch.no_grad():
        output = model(X)
        predictive_mean = output.mean
        predictive_var = output.variance
        error = Y - predictive_mean
        log_likelihoods = -0.5 * torch.log(2 * np.pi * predictive_var) - 0.5 * (error**2) / predictive_var
        lppd = torch.sum(torch.log(torch.mean(torch.exp(log_likelihoods), dim=0)))
        p_waic = torch.sum(torch.var(log_likelihoods, dim=0))
        waic = -2 * (lppd - p_waic)
    return waic.item()

def r2_adj(r2, n, k):
    return 1 - (1 - r2) * (n - 1) / (n - k - 1)

def get_metrics(pred_dist, y_true, y_pred, type="regression"):
    if type == "regression":
        try:
            root_mse = sklearn.metrics.root_mean_squared_error(y_true, y_pred)
        except:
            root_mse = "negative"
        return {
            "mse": sklearn.metrics.mean_squared_error(y_true, y_pred),
            "root_mse": root_mse,
            "mae": sklearn.metrics.mean_absolute_error(y_true, y_pred),
            "mape": sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred),
            "r2": sklearn.metrics.r2_score(y_true, y_pred),
            "r2_adj": r2_adj(sklearn.metrics.r2_score(y_true, y_pred), len(y_true), len(y_pred)),
            "BIC": get_BIC(sklearn.metrics.mean_squared_error(y_true, y_pred), len(y_true), len(y_pred)),
            "AIC": AIC(sklearn.metrics.mean_squared_error(y_true, y_pred), len(y_true), len(y_pred)),
            "explained_variance (ESS)": sklearn.metrics.explained_variance_score(y_true, y_pred),
        }
    elif type == "GP":
        try:
            log_mse = sklearn.metrics.mean_squared_log_error(y_true, y_pred)
        except:
            log_mse = "negative"
        try:
            rmse = sklearn.metrics.root_mean_squared_error(y_true, y_pred)
        except:
            rmse = "negative"
        return {
            "MSE": sklearn.metrics.mean_squared_error(np.array(y_true), np.array(y_pred)),
            #TODO: RMSE is negativ for dummy data
            "mean_squared_log_error": log_mse,
            "RMSE": rmse,
            "MAPE": sklearn.metrics.mean_absolute_percentage_error(y_true, y_pred),
            "r2": sklearn.metrics.r2_score(y_true, y_pred),
            "explained_variance": sklearn.metrics.explained_variance_score(y_true, y_pred),
            "mean_standardized_log_loss": mean_standardized_log_loss(pred_dist, y_true, y_pred),
            "negative_log_predictive_density": negative_log_predictive_density(pred_dist, y_true),
        }
    elif type == "classification":
        return {
            "accuracy": sklearn.metrics.accuracy_score(y_true, y_pred),
            "precision": sklearn.metrics.precision_score(y_true, y_pred),
            "recall": sklearn.metrics.recall_score(y_true, y_pred),
            "f1": sklearn.metrics.f1_score(y_true, y_pred),
            "auc": sklearn.metrics.roc_auc_score(y_true, y_pred),
            "mcc": sklearn.metrics.matthews_corrcoef(y_true, y_pred)
        }

